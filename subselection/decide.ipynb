{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to select a subset\n",
    "\n",
    "Based on different subset sizes (k) and log-transform or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_optimizers = {\n",
    "    \"blackbox\": [\"RandomSearch\", \"SMAC3-BlackBoxFacade\", \"Nevergrad-CMA-ES\"],\n",
    "    \"multi-objective\": [\"RandomSearch\", \"Optuna-MO-TPE\", \"Nevergrad-DE\"],\n",
    "    \"multi-fidelity\": [\"SMAC3-Hyperband\", \"SMAC3-MultiFidelityFacade\", \"DEHB\"],\n",
    "    \"multi-fidelity-objective\": [\"RandomSearch\", \"SMAC3-MOMF-GP\", \"Nevergrad-DE\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from create_subset_configs import fix_legacy_task_id\n",
    "\n",
    "def parse_stats_str(stats_str: str) -> dict:\n",
    "    stats_str = stats_str.strip(\"\\n\").split(\",\")\n",
    "    stats_str = [x.strip(\" \") for x in stats_str]\n",
    "    stats_str = [x.split(\"=\") for x in stats_str]\n",
    "    return {x[0]: literal_eval(x[1]) for x in stats_str}\n",
    "\n",
    "path_task_type = Path(\"data/2024_11\")\n",
    "\n",
    "path_task_type = Path(\"data_subselection/MF/lognorm\")\n",
    "path_task_type = Path(\"data_subselection/MOMF/lognorm\")\n",
    "\n",
    "is_df_crit_log = True\n",
    "scenario = \"momf\"\n",
    "\n",
    "def load_subsets(path_task_type: str | Path, is_df_crit_log: bool, scenario: str) -> pd.DataFrame:\n",
    "    if isinstance(path_task_type, str):\n",
    "        path_task_type = Path(path_task_type)\n",
    "    data = []\n",
    "    # gather old style:\n",
    "    csv_files = [f for f in path_task_type.glob(\"*.csv\") if f.name.startswith(\"subset\")]\n",
    "    for subset_file in csv_files:\n",
    "        subset = pd.read_csv(subset_file)\n",
    "        if \"complement\" in subset_file.name and not \"complement_subset\" in subset_file.name:\n",
    "            continue\n",
    "        if \"problem_id\" in subset.columns:\n",
    "            subset = subset.rename(columns={\"problem_id\": \"task_id\"})\n",
    "\n",
    "        subset_id = \"dev\" if \"complement\" not in subset_file.name else \"test\"\n",
    "\n",
    "        subset[\"task_id\"] = subset[\"task_id\"].apply(fix_legacy_task_id)\n",
    "        task_ids = list(subset[\"task_id\"])\n",
    "        txt_file = subset_file.with_suffix(\".txt\")\n",
    "        if txt_file.exists():\n",
    "            first_line = txt_file.read_text().split(\"\\n\")[0]\n",
    "            stats = parse_stats_str(first_line)\n",
    "        else:\n",
    "            info_csv = subset_file.parent / \"info.csv\"\n",
    "            all_stats = pd.read_csv(info_csv)\n",
    "            stats_subset_id = \"s1\" if subset_id == \"dev\" else \"s2\"\n",
    "            k_from_fn = subset_file.stem.split(\"_\")[-1]\n",
    "            all_stats[\"k\"] = all_stats[\"k\"].astype(int)\n",
    "            stats = all_stats[(all_stats[\"which\"]==stats_subset_id) & (all_stats[\"k\"] == int(k_from_fn))].to_dict(orient=\"records\")[0]\n",
    "            del stats[\"which\"]\n",
    "\n",
    "        info = {\n",
    "            \"subset_id\": subset_id,\n",
    "            \"scenario\": scenario,\n",
    "            \"is_df_crit_log\": is_df_crit_log,\n",
    "            \"task_ids\": task_ids,\n",
    "            **stats,\n",
    "        }\n",
    "        data.append(info)\n",
    "    subset_data = pd.DataFrame(data)\n",
    "    return subset_data\n",
    "\n",
    "\n",
    "def load_subsets_v2():\n",
    "    path = Path(\"data/2024_11\")\n",
    "\n",
    "    columns_bb = [\"task_id\", \"RandomSearch\", \"SMAC3-BlackBoxFacade\", \"Nevergrad-CMA-ES\"]\n",
    "    columns_mo = [\"task_id\", \"RandomSearch\", \"Optuna-MO-TPE\", \"Nevergrad-DE\"]\n",
    "\n",
    "\n",
    "    result_files = list(path.glob(\"*.txt\"))\n",
    "    result_files.sort()\n",
    "\n",
    "    results = []\n",
    "    for result_file in result_files:\n",
    "        with result_file.open() as f:\n",
    "            lines = f.readlines()\n",
    "            # print(lines[0])\n",
    "            subset_raw = \"\".join(lines[2:]).replace(\" \", \",\")\n",
    "            stats_raw = lines[0].strip(\"\\n\").split(\",\")\n",
    "            stats_raw = [x.strip(\" \") for x in stats_raw]\n",
    "            stats_raw = [x.split(\"=\") for x in stats_raw]\n",
    "            stats = {x[0]: literal_eval(x[1]) for x in stats_raw}\n",
    "\n",
    "            stem = result_file.stem\n",
    "            scenario = stem.split(\"_\")[0]\n",
    "            scenario = scenario.replace(\"Mo\", \"MO\")\n",
    "            is_df_crit_log = \"log\" in stem\n",
    "            subset_id = \"test\" if \"TEST\" in stem else \"dev\"\n",
    "            stats[\"subset_id\"] = subset_id\n",
    "            stats[\"scenario\"] = scenario\n",
    "            stats[\"is_df_crit_log\"] = is_df_crit_log\n",
    "            result = {\n",
    "                \"scenario\": scenario,\n",
    "                \"is_df_crit_log\": is_df_crit_log,\n",
    "                \"stats\": stats,\n",
    "                \"subset\": subset_raw,\n",
    "                \"subset_id\": subset_id\n",
    "            }\n",
    "\n",
    "            subset_fn = path / f\"{scenario}_{is_df_crit_log}_{subset_id}_{stats['k']}.csv\"\n",
    "            cols = columns_mo if \"MO\" in scenario else columns_bb\n",
    "            col_line = \",\".join(cols)\n",
    "            subset_raw = col_line + \"\\n\" + subset_raw\n",
    "            with open(subset_fn, \"w\") as f:\n",
    "                f.write(subset_raw)\n",
    "\n",
    "\n",
    "            # print(result)\n",
    "        results.append(result)\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    stats_df = pd.DataFrame(results[\"stats\"].tolist())\n",
    "    stats_df[\"task_ids\"] = results[\"subset\"].apply(lambda x: [l.split(\",\")[0] for l in x.split(\"\\n\") if l])\n",
    "    stats_df[\"task_ids\"] = stats_df[\"task_ids\"].apply(lambda x: [fix_legacy_task_id(xk) for xk in x])\n",
    "    stats_df.loc[stats_df[\"scenario\"] == \"BBv2\", \"scenario\"] = \"blackbox\"\n",
    "    stats_df.loc[stats_df[\"scenario\"] == \"MOv2\", \"scenario\"] = \"multi-objective\"\n",
    "    return stats_df\n",
    "\n",
    "\n",
    "subset_data_mf = load_subsets(\"data_subselection/MF/lognorm\", True, \"multi-fidelity\")\n",
    "subset_data_momf = load_subsets(\"data_subselection/MOMF/lognorm\", True, \"multi-fidelity-objective\")\n",
    "subset_data_bb_mo = load_subsets_v2()\n",
    "\n",
    "subset_data = pd.concat([subset_data_mf, subset_data_momf, subset_data_bb_mo])\n",
    "subset_data.to_csv(\"subset_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Ranks and Significances of Subselections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "from autorank._util import get_sorted_rank_groups\n",
    "from carps.analysis.plot_ranking import calc_critical_difference\n",
    "from create_subset_configs import fix_legacy_task_id\n",
    "\n",
    "# Load source data\n",
    "df_crit_fns = [\n",
    "    \"data/BBv2_norm/df_crit.csv\",\n",
    "    \"data/MOv2_norm/df_crit.csv\",\n",
    "    \"data_subselection/MOMF/default/df_crit.csv\",\n",
    "    \"data_subselection/MF/lognorm/df_crit.csv\",\n",
    "]\n",
    "task_types = [\"blackbox\", \"multi-objective\", \"multi-fidelity-objective\", \"multi-fidelity\"]\n",
    "df_crit = pd.concat([pd.read_csv(fn).melt(id_vars=[\"problem_id\"], var_name=\"optimizer_id\", value_name=\"performance\") for fn in df_crit_fns]).rename({\"problem_id\": \"task_id\"}, axis=1)\n",
    "df_crit[\"task_id\"] = df_crit[\"task_id\"].apply(fix_legacy_task_id)\n",
    "# df_crit = df_crit.set_index(\"task_id\")\n",
    "df_crit.to_csv(\"tmp_df_crit.csv\", index=False)\n",
    "\n",
    "subset_data = pd.read_csv(\"subset_data.csv\")\n",
    "\n",
    "def calc_ranks(rundata: pd.DataFrame, scenario: str, set_id: str):\n",
    "    perf_col: str = \"trial_value__cost_inc_norm\"\n",
    "    identifier = f\"{scenario}_{set_id}\"\n",
    "    result = calc_critical_difference(rundata, identifier=identifier, figsize=(8, 3), perf_col=perf_col, calc_df_crit=False, plot_diagram=False)\n",
    "    sorted_ranks, names, groups = get_sorted_rank_groups(result, reverse=False)\n",
    "    return result\n",
    "\n",
    "decision_data = []\n",
    "for (scenario, is_df_crit_log, k), gdf in subset_data.groupby([\"scenario\", \"is_df_crit_log\", \"k\"]):\n",
    "    discrepancy_sum = gdf[\"discrepancy\"].sum()\n",
    "\n",
    "    is_significant = {}\n",
    "    rank = {}\n",
    "    rankorder = {}\n",
    "    for set_id, sgdf in gdf.groupby(\"subset_id\"):\n",
    "        allowed_optimizers = original_optimizers[scenario]\n",
    "        allowed_task_ids = ast.literal_eval(sgdf[\"task_ids\"].iloc[0])\n",
    "        rundata_subset = df_crit[\n",
    "            # (rundata[\"task_type\"] == scenario)\n",
    "            (df_crit[\"optimizer_id\"].isin(allowed_optimizers))\n",
    "            & (df_crit[\"task_id\"].isin(allowed_task_ids))        ]\n",
    "        rundata_subset = rundata_subset.pivot_table(index=\"task_id\", columns=\"optimizer_id\", values=\"performance\")\n",
    "        ranks = calc_ranks(rundata_subset, scenario, set_id)\n",
    "        # from rich import inspect\n",
    "        # inspect(ranks)\n",
    "        is_significant[\"is_significant_\" + set_id] = bool(ranks.pvalue < ranks.alpha)\n",
    "        rank[\"rank_\" + set_id] = ranks.rankdf[\"meanrank\"]\n",
    "        rankorder[\"rankorder_\" + set_id] = ranks.rankdf[\"meanrank\"].rank(ascending=True)\n",
    "    rankorder_is_same = rankorder[\"rankorder_test\"].equals(rankorder[\"rankorder_dev\"])\n",
    "    decision_data.append({\n",
    "        \"scenario\": scenario,\n",
    "        \"is_df_crit_log\": is_df_crit_log,\n",
    "        \"k\": k,\n",
    "        \"discrepancy_sum\": discrepancy_sum,\n",
    "        \"rankorder_is_same\": rankorder_is_same,\n",
    "        **is_significant\n",
    "    })\n",
    "decision_df = pd.DataFrame(decision_data)\n",
    "decision_df.to_csv(\"decision_data.csv\", index=False)\n",
    "decision_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_df = pd.read_csv(\"decision_data.csv\")\n",
    "\n",
    "# Requirements/Filtering:\n",
    "# (1) The rank order must stay the same between dev and test.\n",
    "# (2) Dev and test must show significant differences based on the non-parametric test.\n",
    "decision_df = decision_df[\n",
    "    (decision_df[\"rankorder_is_same\"] == True)\n",
    "    & (decision_df[\"is_significant_dev\"] == True)\n",
    "    & (decision_df[\"is_significant_test\"] == True)\n",
    "]\n",
    "\n",
    "# Decision Rule: Pick the k and log transformation with the lowest discrepancy sum of dev and test.\n",
    "choices = decision_df.groupby(\"scenario\").apply(lambda gdf: gdf[gdf[\"discrepancy_sum\"] == gdf[\"discrepancy_sum\"].min()])\n",
    "\n",
    "# Add task ids from subset_data\n",
    "for idx, row in choices.iterrows():\n",
    "    scenario = row[\"scenario\"]\n",
    "    is_df_crit_log = row[\"is_df_crit_log\"]\n",
    "    k = row[\"k\"]\n",
    "    gdf = subset_data[(subset_data[\"scenario\"] == scenario) & (subset_data[\"is_df_crit_log\"] == is_df_crit_log) & (subset_data[\"k\"] == k)]\n",
    "    for set_id in [\"dev\", \"test\"]:\n",
    "        task_ids = gdf[gdf[\"subset_id\"] == set_id][\"task_ids\"].iloc[0]\n",
    "        choices.at[idx, f\"task_ids_{set_id}\"] = task_ids\n",
    "choices = choices.reset_index(drop=True)\n",
    "\n",
    "choices.to_csv(\"choices.csv\", index=False)\n",
    "choices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset configs\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import fire\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def fix_legacy_task_id(task_id: str) -> str:\n",
    "    task_id = \"bbob/\" + task_id if task_id.startswith(\"noiseless\") else task_id\n",
    "    return task_id.replace(\n",
    "         \"noiseless/\", \"\").replace(\n",
    "              \"bb/tab/\", \"blackbox/tabular/\").replace(\n",
    "                   \"MO/tab/\", \"multiobjective/tabular/\").replace(\"hpobench/mf/\", \"hpobench/multifidelity/\")\n",
    "\n",
    "def write_subset(task_ids: list[str], subset_id: str, scenario: str, config_target_path: str | Path = \"carps/configs/task/subselection\") -> None:\n",
    "    config_target_path = (Path(config_target_path) / scenario.replace(\"-\", \"\")).resolve()\n",
    "    config_target_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    task_ids = [fix_legacy_task_id(task_id) for task_id in task_ids]\n",
    "\n",
    "    subset_size = len(task_ids)\n",
    "\n",
    "    index_fn = config_target_path.parent.parent / \"index.csv\"\n",
    "    if not index_fn.is_file():\n",
    "        raise ValueError(f\"Could not find {index_fn}. ObjectiveFunction ids have not been indexed. Run `python -m carps.utils.index_configs`.\")\n",
    "    task_index = pd.read_csv(index_fn)\n",
    "    not_found = [pid for pid in task_ids if pid not in task_index[\"task_id\"].to_list()]\n",
    "    if not_found:\n",
    "            raise ValueError(f\"Could not find {not_found} in {index_fn}. ObjectiveFunction ids have not been indexed. Run `python -m carps.utils.index_configs`.\")\n",
    "\n",
    "    ids = [np.where(task_index[\"task_id\"]==pid)[0][0] for pid in task_ids]\n",
    "    config_fns = task_index[\"config_fn\"][ids].to_list()\n",
    "\n",
    "    for config_fn in config_fns:\n",
    "        cfg = OmegaConf.load(config_fn)\n",
    "        new_name = f\"subset_{cfg.task_id}.yaml\".replace(\"/\", \"_\")\n",
    "        new_task_id = f\"{scenario}/{subset_size}/{subset_id}/{cfg.task_id}\"\n",
    "        cfg.task_id = new_task_id\n",
    "        new_fn = config_target_path / subset_id / new_name\n",
    "        new_fn.parent.mkdir(exist_ok=True, parents=True)\n",
    "        yaml_str = OmegaConf.to_yaml(cfg)\n",
    "        yaml_str = f\"# @package _global_\\ntask_type: {scenario}\\nsubset_id: {subset_id}\\n\" + yaml_str\n",
    "        new_fn.write_text(yaml_str)\n",
    "\n",
    "\n",
    "choices = pd.read_csv(\"choices.csv\")\n",
    "choices[\"task_ids_dev\"] = choices[\"task_ids_dev\"].apply(ast.literal_eval)\n",
    "choices[\"task_ids_test\"] = choices[\"task_ids_test\"].apply(ast.literal_eval)\n",
    "for _, row in choices.iterrows():\n",
    "    print(row)\n",
    "    write_subset(row[\"task_ids_dev\"], \"dev\", row[\"scenario\"], config_target_path=\"../carps/configs/task/subselection\")\n",
    "    write_subset(row[\"task_ids_test\"], \"test\", row[\"scenario\"], config_target_path=\"../carps/configs/task/subselection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carpsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
