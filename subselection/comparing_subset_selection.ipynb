{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Subset Selection Methods\n",
    "Compare discprency based subselection to easiest/hardest tasks (determined via the Pareto front)\n",
    "\n",
    "## Prerequisite\n",
    "Full data for each scenario needed and the results from the subselection procedure.\n",
    "\n",
    "## Limitations\n",
    "Using the subset size of the discrepancy subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Scenario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "scenario = \"blackbox\"\n",
    "set_id = \"full\"\n",
    "subset_size = 30\n",
    "paths = [\"../runs/SMAC3-BlackBoxFacade\", \"../runs/RandomSearch\", \"../runs/Nevergrad-CMA-ES\"]\n",
    "performance_fn = \"/scratch/hpc-prf-intexml/cbenjamins/repos/CARP-S-Experiments/lib/CARP-S/subselection/data/BB/default/df_crit.csv\"\n",
    "\n",
    "# scenario = \"multi-fidelity-objective\"\n",
    "# set_id = \"full\"\n",
    "# subset_size = 9\n",
    "# paths = [\"../runs_MOMF/SMAC3-MOMF-GP\", \"../runs_MOMF/RandomSearch\", \"../runs_MOMF/Nevergrad-DE\"]\n",
    "# performance_fn = \"/scratch/hpc-prf-intexml/cbenjamins/repos/CARP-S-Experiments/lib/CARP-S/subselection/data/MOMF/lognorm/df_crit.csv\"\n",
    "\n",
    "# scenario = \"multi-objective\"\n",
    "# set_id = \"full\"\n",
    "# subset_size = 10\n",
    "# paths = [\"../runs_MO/Optuna-MO\", \"../runs_MO/RandomSearch\", \"../runs_MO/Nevergrad-DE\"]\n",
    "# performance_fn = \"/scratch/hpc-prf-intexml/cbenjamins/repos/CARP-S-Experiments/lib/CARP-S/subselection/data/MO/lognorm/df_crit.csv\"\n",
    "\n",
    "# scenario = \"multi-fidelity\"\n",
    "# set_id = \"full\"\n",
    "# subset_size = 20\n",
    "# paths = [\"../runs/SMAC3-Hyperband\", \"../runs/SMAC3-MultiFidelityFacade\", \"../runs_/DEHB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from carps.analysis.gather_data import normalize_logs\n",
    "from carps.analysis.run_autorank import get_df_crit\n",
    "from carps.analysis.utils import filter_only_final_performance\n",
    "\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "original_optimizers = {\n",
    "    \"blackbox\": [\"RandomSearch\", \"SMAC3-BlackBoxFacade\", \"Nevergrad-CMA-ES\"],\n",
    "    \"multi-objective\": [\"RandomSearch\", \"Optuna-MO\", \"Nevergrad-DE\"],\n",
    "    \"multi-fidelity\": [\"SMAC3-Hyperband\", \"SMAC3-MultiFidelityFacade\", \"DEHB\"],\n",
    "    \"multi-fidelity-objective\": [\"RandomSearch\", \"SMAC3-MOMF-GP\", \"Nevergrad-DE\"],\n",
    "}\n",
    "\n",
    "def load_set(paths: list[str], set_id: str = \"unknown\") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    logs = []\n",
    "    for p in paths:\n",
    "        fn = Path(p) / \"trajectory.parquet\"\n",
    "        if not fn.is_file():\n",
    "            fn = Path(p) / \"logs.parquet\"\n",
    "        logs.append(pd.read_parquet(fn))\n",
    "\n",
    "    df = pd.concat(logs).reset_index(drop=True)\n",
    "    df_cfg = pd.concat([pd.read_parquet(Path(p) / \"logs_cfg.parquet\") for p in paths]).reset_index(drop=True)\n",
    "    df[\"set\"] = set_id\n",
    "    return df, df_cfg\n",
    "\n",
    "fn = f\"../data/{scenario}_{set_id}_logs.parquet\"\n",
    "if Path(fn).is_file():\n",
    "    df = pd.read_parquet(fn)\n",
    "else:\n",
    "    D = []\n",
    "    for rundir in paths:\n",
    "        df, df_cfg = load_set([rundir], set_id=\"full\")\n",
    "        D.append(df)\n",
    "\n",
    "    df = pd.concat(D).reset_index(drop=True)\n",
    "    del D\n",
    "\n",
    "    df = normalize_logs(df)\n",
    "\n",
    "    normalize_performance = False\n",
    "    perf_col = \"trial_value__cost_inc_norm\" if normalize_performance else \"trial_value__cost_inc\"\n",
    "\n",
    "    df.to_parquet(f\"../data/{scenario}_{set_id}_logs.parquet\")\n",
    "df = filter_only_final_performance(df=df)\n",
    "df_crit = get_df_crit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate subsets\n",
    "\n",
    "1. Random I: randomly sample from full set of tasks\n",
    "2. Random II: randomly sample from full set of tasks, but keep fraction per benchmark the same\n",
    "3. Easiest: Find easiest tasks with non-dominated sorting, select top-k\n",
    "4. Hardest: Find hardest tasks with non-dominated sorting, select top-k\n",
    "\n",
    "For all of them: Divide them into dev and test.\n",
    "For the random subsets: Create n splits.\n",
    "As this is stochastic, repeat the procedure for 5 seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from carps.utils.pareto_front import pareto\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "def read_set(fn: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fn)\n",
    "    df[\"task_id\"] = df[\"task_id\"].apply(lambda x: \"bbob/\" + x if x.startswith(\"noiseless\") else x)\n",
    "    df[\"benchmark_id\"] = df[\"task_id\"].apply(lambda x: x.split(\"/\")[0])\n",
    "    return df.melt(id_vars=[\"task_id\", \"benchmark_id\"], value_vars=original_optimizers[scenario], var_name=\"optimizer_id\", value_name=\"performance\")\n",
    "\n",
    "task_ids = df[\"task_id\"].unique()\n",
    "# print(task_ids)\n",
    "benchmark_ids = [pid.split(\"/\")[0] for pid in task_ids]\n",
    "n_tasks = len(task_ids)\n",
    "\n",
    "performance_fn = Path(performance_fn)\n",
    "path_subset_dev = performance_fn.parent / f\"subset_{subset_size}.csv\"\n",
    "subset_dev = read_set(path_subset_dev)\n",
    "\n",
    "path_subset_test = performance_fn.parent / f\"subset_complement_subset_{subset_size}.csv\"\n",
    "subset_test = read_set(path_subset_test)\n",
    "\n",
    "task_ids_dev = subset_dev[\"task_id\"].to_list()\n",
    "task_ids_test = subset_test[\"task_id\"].to_list()\n",
    "\n",
    "print(n_tasks)\n",
    "\n",
    "\n",
    "\n",
    "seeds = np.arange(0, 5)\n",
    "\n",
    "split_classes = [StratifiedShuffleSplit, ShuffleSplit]\n",
    "\n",
    "def get_set_type(x: str) -> str:\n",
    "    if x.startswith(\"class\"):\n",
    "        _id = \"StratifiedShuffleSplit\" if \"StratifiedShuffleSplit\" in x else \"ShuffleSplit\"\n",
    "        if x.endswith(\"dev\"):\n",
    "            return f\"{_id}_dev\"\n",
    "        return f\"{_id}_test\"\n",
    "    if x.startswith(\"pareto\"):\n",
    "        _id = \"pareto\"\n",
    "        if x.endswith(\"dev\"):\n",
    "            return f\"{_id}_dev\"\n",
    "        return f\"{_id}_test\"\n",
    "    return x\n",
    "\n",
    "new_subset_performance = []\n",
    "\n",
    "\n",
    "\n",
    "def find_pareto(df_crit: pd.DataFrame, which: str = \"easiest\") -> pd.DataFrame:\n",
    "    n_tries = 1000\n",
    "    counter = 0\n",
    "    index = []\n",
    "    while len(df_crit) > 0:\n",
    "        crit_points = df_crit.values\n",
    "        sign = 1 if which == \"easiest\" else -1\n",
    "        ids_pareto = pareto(sign * crit_points)\n",
    "        index.extend(df_crit[ids_pareto].index)\n",
    "        df_crit = df_crit[~ids_pareto]\n",
    "\n",
    "        if counter > n_tries:\n",
    "            break\n",
    "        counter += 1\n",
    "    return index\n",
    "\n",
    "# Pareto\n",
    "print(\"Pareto\")\n",
    "df_crit_ = get_df_crit(df[df[\"task_id\"].isin(task_ids)], perf_col=\"trial_value__cost_inc_norm\")\n",
    "task_ids_easiest = find_pareto(df_crit_, which=\"easiest\")[:int(subset_size * 2)]\n",
    "task_ids_hardest = find_pareto(df_crit_, which=\"hardest\")[:int(subset_size * 2)]\n",
    "\n",
    "split_class = ShuffleSplit\n",
    "for set_origin, pids in zip([\"pareto_easiest\", \"pareto_hardest\"], [task_ids_easiest, task_ids_hardest], strict=False):\n",
    "    bids = [pid.split(\"/\")[0] for pid in pids]\n",
    "    X = pids\n",
    "    y = bids\n",
    "    n_splits = len(pids) // (subset_size * 2)\n",
    "\n",
    "    for seed in seeds:\n",
    "        sss = split_class(n_splits=n_splits, train_size=subset_size/len(pids), test_size=subset_size/len(pids), random_state=seed)\n",
    "        sss.get_n_splits(X, y)\n",
    "        for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "\n",
    "\n",
    "            assert len(train_index) == subset_size\n",
    "\n",
    "            ids_dev = task_ids[train_index]\n",
    "            ids_test = task_ids[test_index]\n",
    "            for _set_id, ids in zip([\"dev\", \"test\"], (ids_dev, ids_test), strict=False):\n",
    "                new_df = df[df[\"task_id\"].isin(ids)].copy()\n",
    "                set_id = f\"{set_origin}_class_{split_class.__name__}_split_{i}_seed_{seed}_subset_{_set_id}\"\n",
    "                new_df[\"set\"] = set_id\n",
    "                print(set_id, len(train_index), len(test_index), new_df[\"task_id\"].nunique())\n",
    "                new_subset_performance.append(new_df)\n",
    "\n",
    "\n",
    "print(\"Standard\")\n",
    "for set_id, ids in zip([\"full\", \"discrepancy_subset_dev\", \"discrepancy_subset_test\"], (task_ids, task_ids_dev, task_ids_test), strict=False):\n",
    "    new_df = df[df[\"task_id\"].isin(ids)].copy()\n",
    "    new_df[\"set\"] = set_id\n",
    "    new_subset_performance.append(new_df)\n",
    "\n",
    "# Random Splits\n",
    "print(\"Random\")\n",
    "n_splits = n_tasks // (subset_size * 2)\n",
    "# n_splits = 1\n",
    "print(n_splits)\n",
    "X = task_ids\n",
    "y = benchmark_ids\n",
    "for split_class in split_classes:\n",
    "    for seed in seeds:\n",
    "        sss = split_class(n_splits=n_splits, train_size=subset_size/n_tasks, test_size=subset_size/n_tasks, random_state=seed)\n",
    "        sss.get_n_splits(X, y)\n",
    "        for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "\n",
    "\n",
    "            assert len(train_index) == subset_size\n",
    "\n",
    "            ids_dev = task_ids[train_index]\n",
    "            ids_test = task_ids[test_index]\n",
    "            for _set_id, ids in zip([\"dev\", \"test\"], (ids_dev, ids_test), strict=False):\n",
    "                new_df = df[df[\"task_id\"].isin(ids)].copy()\n",
    "                set_id = f\"class_{split_class.__name__}_split_{i}_seed_{seed}_subset_{_set_id}\"\n",
    "                new_df[\"set\"] = set_id\n",
    "                print(set_id, len(train_index), len(test_index), new_df[\"task_id\"].nunique())\n",
    "                new_subset_performance.append(new_df)\n",
    "\n",
    "df_new = pd.concat(new_subset_performance).reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_type(x: str) -> str:\n",
    "    if x.startswith(\"class\"):\n",
    "        _id = \"StratifiedShuffleSplit\" if \"StratifiedShuffleSplit\" in x else \"ShuffleSplit\"\n",
    "        if x.endswith(\"dev\"):\n",
    "            return f\"{_id}_dev\"\n",
    "        return f\"{_id}_test\"\n",
    "    if x.startswith(\"pareto\"):\n",
    "        _id = \"pareto\"\n",
    "        if x.endswith(\"dev\"):\n",
    "            return f\"{_id}_dev\"\n",
    "        return f\"{_id}_test\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average seeds\n",
    "Get data: performance on optimizer_id/task_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crit = {}\n",
    "for set_id, gdf in df_new.groupby(\"set\"):\n",
    "    df_crit[set_id] = get_df_crit(gdf, perf_col=\"trial_value__cost_inc_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crit_points = {k: v[original_optimizers[scenario]].values for k, v in df_crit.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and Plot Discrepancy\n",
    "Note: This is the WD discrepancy, not exactly the one used for the subselection.\n",
    "It should be precise enough however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import qmc\n",
    "\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "discprecancies = {}\n",
    "for set_id, points in df_crit_points.items():\n",
    "    # print(set_id, points.shape)\n",
    "    discprecancies[set_id] = qmc.discrepancy(points, method=\"WD\")\n",
    "discrepancies = pd.Series(discprecancies)\n",
    "discrepancies.index.name = \"set\"\n",
    "discrepancies.name = \"discrepancy\"\n",
    "discrepancies = discrepancies.reset_index()\n",
    "discrepancies[\"set_type\"] = discrepancies[\"set\"].apply(get_set_type)\n",
    "discrepancies\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(4, 2))\n",
    "ax = fig.add_subplot(111)\n",
    "ax = sns.scatterplot(data=discrepancies, x=\"discrepancy\", y=\"set_type\", ax=ax)\n",
    "# ax.set_yscale(\"log\")\n",
    "# ax.tick_params(axis='x', rotation=90)\n",
    "ax.set_title(\"discrepancy\")\n",
    "ax.figure.savefig(f\"../data/{scenario}_discrepancy.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc Ranks + Count How Often Different\n",
    "Calc ranks with statistical tests/autorank and count how often the ranking differs between the dev and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "from autorank._util import get_sorted_rank_groups\n",
    "from carps.analysis.plot_ranking import calc_critical_difference\n",
    "\n",
    "rank_results = {}\n",
    "for (scenario, set_id), gdf in _df.groupby(by=[\"scenario\", \"set\"]):\n",
    "    perf_col: str = \"trial_value__cost_inc_norm\"\n",
    "    identifier = f\"{scenario}_{set_id}\"\n",
    "    result = calc_critical_difference(gdf, identifier=identifier, figsize=(8, 3), perf_col=perf_col, plot_diagram=False)\n",
    "    sorted_ranks, names, groups = get_sorted_rank_groups(result, reverse=False)\n",
    "    rank_results[(scenario, set_id)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = []\n",
    "for i, (k, v) in enumerate(rank_results.items()):\n",
    "    d = pd.DataFrame({\n",
    "        \"scenario\": k[0],\n",
    "        \"set\": k[1],\n",
    "        **v.rankdf[\"meanrank\"]\n",
    "    }, index=[i]\n",
    "    ).melt(id_vars=[\"scenario\", \"set\"], var_name=\"optimizer_id\", value_name=\"meanrank\")\n",
    "    d[\"order\"] = d.rank(method=\"max\", numeric_only=True).astype(int)\n",
    "    R.append(d)\n",
    "    # break\n",
    "\n",
    "df_rank = pd.concat(R).reset_index(drop=True)\n",
    "df_rank = df_rank[df_rank[\"set\"] != \"full\"]\n",
    "df_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "is_different = {}\n",
    "for scenario, gdf in df_rank.groupby(by=\"scenario\"):\n",
    "    set_origins = gdf[\"set\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-2])).unique()\n",
    "    for set_origin in set_origins:\n",
    "        origs = original_optimizers[scenario]\n",
    "        df_dev = gdf[gdf[\"set\"] == f\"{set_origin}_subset_dev\"]\n",
    "        df_test = gdf[gdf[\"set\"] == f\"{set_origin}_subset_test\"]\n",
    "        order_dev = []\n",
    "        order_test = []\n",
    "        for orig in origs:\n",
    "            order_dev.append(df_dev[df_dev[\"optimizer_id\"] == orig][\"order\"].values[0])\n",
    "            order_test.append(df_test[df_test[\"optimizer_id\"] == orig][\"order\"].values[0])\n",
    "        _is_different = order_dev != order_test\n",
    "        is_different[(scenario, set_origin)] = _is_different\n",
    "is_different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = pd.Series({k[1]: v for k, v in is_different.items()})\n",
    "df_diff.index.name = \"origin_detail\"\n",
    "df_diff.name = \"is_different\"\n",
    "df_diff = df_diff.reset_index()\n",
    "df_diff[\"is_different\"] = df_diff[\"is_different\"].astype(int)\n",
    "def get_origin(x: str) -> str:\n",
    "    if x.startswith(\"class\"):\n",
    "        return x.split(\"_\")[1]\n",
    "    if x.startswith(\"pareto\"):\n",
    "        return \"pareto\"\n",
    "    return x\n",
    "df_diff[\"origin\"] = df_diff[\"origin_detail\"].apply(get_origin)\n",
    "df_diff\n",
    "\n",
    "df_diff_reduced = df_diff.groupby(by=\"origin\").agg({\"is_different\": \"sum\"}).reset_index()\n",
    "n_total_splits = n_splits * len(seeds)\n",
    "df_diff_reduced[\"fraction_is_different\"] = df_diff_reduced[\"is_different\"] / n_total_splits\n",
    "df_diff_reduced.to_latex(f\"../data/{scenario}_is_different.tex\", index=False)\n",
    "df_diff_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Print some tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fn_template = \"ranks_per_set_{scenario}_{set_origin}.csv\"\n",
    "\n",
    "decimal_places = 2\n",
    "\n",
    "final_str = r\"\"\"\n",
    "\\begin{{table}}[h]\n",
    "    \\caption{{{caption}}}\n",
    "    \\label{{{label}}}\n",
    "    \\centering\n",
    "    %\\resizebox{{0.4\\textwidth}}{{!}}{{\n",
    "    {table_string}\n",
    "    %}}\n",
    "\\end{{table}}\n",
    "\"\"\"\n",
    "\n",
    "float_format = lambda x: (\"{:0.\" + str(decimal_places) + \"f}\").format(x) if not np.isnan(x) else \"-\"\n",
    "\n",
    "\n",
    "for scenario, gdf in df_rank.groupby(\"scenario\"):\n",
    "    set_origins = gdf[\"set\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-2])).unique()\n",
    "    for set_origin in set_origins:\n",
    "        fn = fn_template.format(scenario=scenario, set_origin=set_origin)\n",
    "\n",
    "\n",
    "        sorter = gdf[gdf[\"set\"] == set_origin + \"_subset_dev\"].sort_values(\"meanrank\")[\"optimizer_id\"].to_list()\n",
    "\n",
    "        _gdf = gdf[gdf[\"set\"].apply(lambda x: x.startswith(set_origin))].copy()\n",
    "        R = _gdf.pivot_table(index=\"set\", columns=\"optimizer_id\", values=\"order\").map(int)\n",
    "        origs = original_optimizers[scenario]\n",
    "        origs.sort(key=lambda x: sorter.index(x))\n",
    "        cols = origs + [c for c in R.columns if c not in original_optimizers[scenario]]\n",
    "        R = R[cols]\n",
    "\n",
    "\n",
    "        MR = _gdf.pivot_table(index=\"set\", columns=\"optimizer_id\", values=\"meanrank\").map(lambda x: f\"{x:.2f}\" if not isinstance(x, str) else x)\n",
    "        MR = MR[cols]\n",
    "        for i, ((_idx, row), (_idx2, row2)) in enumerate(zip(MR.iterrows(), R.iterrows(), strict=False)):\n",
    "            for j in range(len(row)):\n",
    "                row.iloc[j] = row.iloc[j] + f\" ({int(row2.iloc[j])})\"\n",
    "\n",
    "        print(MR)\n",
    "        # table_str = MR.to_latex(float_format=float_format, na_rep=\"-\").strip()\n",
    "        # caption = f\"Mean Ranking for Scenario {scenario}\"\n",
    "        # label = f\"tab:ranking_validation_{scenario}\"\n",
    "        # table_str = final_str.format(table_string=table_str, label=label, caption=caption)\n",
    "        # table_str = table_str.replace(\"_\", \"\\_\")\n",
    "\n",
    "        # with open(fn + \".tex\", \"w\") as file:\n",
    "        #     file.write(table_str)\n",
    "        # print(table_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carpsexp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
