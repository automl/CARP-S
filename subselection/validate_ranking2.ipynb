{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pandas as pd\n",
    "from carps.analysis.gather_data import normalize_logs, rename_legacy\n",
    "\n",
    "original_optimizers = {\n",
    "    \"blackbox\": [\"RandomSearch\", \"SMAC3-BlackBoxFacade\", \"Nevergrad-CMA-ES\"],\n",
    "    \"multi-objective\": [\"RandomSearch\", \"Optuna-MO-TPE\", \"Nevergrad-DE\"],\n",
    "    \"multi-fidelity\": [\"SMAC3-Hyperband\", \"SMAC3-MultiFidelityFacade\", \"DEHB\"],\n",
    "    \"multi-fidelity-objective\": [\"RandomSearch\", \"SMAC3-MOMF-GP\", \"Nevergrad-DE\"],\n",
    "}\n",
    "\n",
    "original_optimizers_only = True\n",
    "origins = {\n",
    "    \"full\": \"full\",\n",
    "    \"dev\": \"discrepancy\",\n",
    "    \"test\": \"discrepancy\",\n",
    "}\n",
    "\n",
    "df = pd.read_parquet(\"../rundata.parquet\")\n",
    "print(df[\"scenario\"].unique())\n",
    "print(df.groupby(\"scenario\").apply(lambda gdf: gdf[\"optimizer_id\"].unique()))\n",
    "if original_optimizers_only:\n",
    "    D = []\n",
    "    for scenario, gdf in df.groupby(\"scenario\"):\n",
    "        gdf = gdf[gdf[\"optimizer_id\"].isin(original_optimizers[scenario])]\n",
    "        D.append(gdf)\n",
    "    df = pd.concat(D).reset_index(drop=True)\n",
    "df = rename_legacy(df)\n",
    "df = normalize_logs(df)\n",
    "df[\"origin\"] = df[\"set\"].apply(lambda x: origins[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autorank._util import get_sorted_rank_groups\n",
    "from carps.analysis.plot_ranking import calc_critical_difference\n",
    "\n",
    "rank_results = {}\n",
    "for (scenario, set_id), gdf in df.groupby(by=[\"task_type\", \"set\"]):\n",
    "    print(scenario, set_id)\n",
    "    print(gdf[\"optimizer_id\"].unique())\n",
    "    perf_col: str = \"trial_value__cost_inc_norm\"\n",
    "    identifier = f\"{scenario}_{set_id}\"\n",
    "    result = calc_critical_difference(gdf, identifier=identifier, figsize=(8, 3), perf_col=perf_col)\n",
    "    sorted_ranks, names, groups = get_sorted_rank_groups(result, reverse=False)\n",
    "    avg_performance = gdf.groupby(\"optimizer_id\")[perf_col].mean().sort_values()\n",
    "    rank_results[(scenario, set_id)] = (result, avg_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_order = \"meanrank\"\n",
    "from rich import inspect\n",
    "\n",
    "# R = {k: v[\"mearn_rank\"] for k, v in rank_results.items()}\n",
    "R = []\n",
    "for i, (k, v) in enumerate(rank_results.items()):\n",
    "    rank_result, avg_performance = v\n",
    "    # inspect(rank_result)\n",
    "    perf = pd.DataFrame([rank_result.rankdf[\"meanrank\"], avg_performance]).T\n",
    "    perf = perf.rename(columns={\"trial_value__cost_inc_norm\": \"trial_value__cost_inc_norm_avg\"})\n",
    "    perf[\"scenario\"] = k[0]\n",
    "    perf[\"set_id\"] = k[1]\n",
    "    perf[\"is_not_significant\"] = rank_result.pvalue >= rank_result.alpha\n",
    "    perf.index.name = \"optimizer_id\"\n",
    "    perf = perf.reset_index()\n",
    "    # perf.index = [i]\n",
    "    # del perf[\"trial_value__cost_inc_norm_avg\"]\n",
    "    perf[\"order\"] = perf[key_order].rank(method=\"max\", numeric_only=True).astype(int)\n",
    "    # d = perf.melt(id_vars=[\"scenario\", \"set_id\"], var_name=\"optimizer_id\", value_name=\"meanrank\")\n",
    "    # d[\"order\"] = d.rank(method=\"max\", numeric_only=True).astype(int)\n",
    "    # R.append(d)\n",
    "    R.append(perf)\n",
    "    # break\n",
    "\n",
    "df_rank = pd.concat(R).reset_index(drop=True)\n",
    "df_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fn_template = \"ranks_per_set_{scenario}.csv\"\n",
    "\n",
    "decimal_places = 2\n",
    "\n",
    "final_str = r\"\"\"\n",
    "\\begin{{table}}[h]\n",
    "    \\caption{{{caption}}}\n",
    "    \\label{{{label}}}\n",
    "    \\centering\n",
    "    %\\resizebox{{0.4\\textwidth}}{{!}}{{\n",
    "    {table_string}\n",
    "    %}}\n",
    "\\end{{table}}\n",
    "\"\"\"\n",
    "\n",
    "float_format = lambda x: (\"{:0.\" + str(decimal_places) + \"f}\").format(x) if not np.isnan(x) else \"-\"\n",
    "\n",
    "\n",
    "for scenario, gdf in df_rank.groupby(\"scenario\"):\n",
    "    fn = fn_template.format(scenario=scenario)\n",
    "    sorter = gdf[gdf[\"set_id\"]==\"dev\"].sort_values(key_order)[\"optimizer_id\"].to_list()\n",
    "\n",
    "    R = gdf.pivot_table(index=\"set_id\", columns=\"optimizer_id\", values=\"order\").map(int)\n",
    "    origs = original_optimizers[scenario]\n",
    "    origs.sort(key=lambda x: sorter.index(x))\n",
    "    cols = origs + [c for c in R.columns if c not in original_optimizers[scenario]]\n",
    "    R = R[cols]\n",
    "\n",
    "\n",
    "    MR = gdf.pivot_table(index=\"set_id\", columns=\"optimizer_id\", values=\"meanrank\").map(lambda x: f\"{x:.2f}\" if not isinstance(x, str) else x)\n",
    "    MR = MR[cols]\n",
    "    for i, ((_idx, row), (_idx2, row2)) in enumerate(zip(MR.iterrows(), R.iterrows(), strict=False)):\n",
    "        for j in range(len(row)):\n",
    "            row.iloc[j] = row.iloc[j] + f\" ({int(row2.iloc[j])})\"\n",
    "\n",
    "    def get_significance_str(dataframe) -> str:\n",
    "        if dataframe[\"is_not_significant\"].all():\n",
    "            return \"no\"\n",
    "        return \"yes\"\n",
    "\n",
    "    MR[\"significant\"] = gdf.groupby(\"set_id\").apply(get_significance_str)\n",
    "\n",
    "    table_str = MR.to_latex(float_format=float_format, na_rep=\"-\").strip()\n",
    "    caption = f\"Mean Ranking for Scenario {scenario}\"\n",
    "    if gdf[\"is_not_significant\"].all():\n",
    "        caption += \" (non-significant)\"\n",
    "    label = f\"tab:ranking_validation_{scenario}\"\n",
    "    table_str = table_str.replace(\"_\", r\"\\_\")\n",
    "    table_str = final_str.format(table_string=table_str, label=label, caption=caption)\n",
    "\n",
    "    with open(fn + \".tex\", \"w\") as file:\n",
    "        file.write(table_str)\n",
    "    print(table_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carpsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
