{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#carp-s","title":"CARP-S","text":"<p>Welcome to the CARP-S documentation.</p> <p>Tip</p> <p>See the navigation links in the header or side-bars. Click the  button (top left) on mobile.</p> <p>For information on how to install CARP-S, please refer to the installation instructions. To see an example of how to run a first optimizer on a benchmark, as well as a more comprehensive overview of what CARP-S can offer, please refer to our guides. If you are interested in the commands that CARP-S provides, please refer to the commands. To learn how to add your own optimizer or benchmark, or if you want to contribute to CARP-S in general,  check out our contributing section.</p>"},{"location":"#what-is-carp-s","title":"What is CARP-S?","text":"<p>CARP-S is a benchmarking framework for Comprehensive Automated Research Performance Studies,  designed to evaluate multiple optimizers across various benchmark tasks. It aims to facilitate  the development of effective and reliable methods for hyperparameter optimization (HPO).</p> <ul> <li> <p> Develop With Confidence</p> <p>Developing optimizers is hard, evaluating them is harder and benchmarking against state-of-the-art is even harder. CARP-S provides a simple and easily extendable framework to develop, evaluate and benchmark your optimizer.</p> </li> <li> <p> Various Scenarios</p> <p>The benchmarks integrated in CARP-S offer tasks for blackbox, multi-fidelity,  multi-objective and multi-fidelity-multi-objective HPO, allowing you to evaluate your optimizer in a variety of scenarios.</p> </li> <li> <p> Effective Evaluation</p> <p>Fair comparisons between optimizers require evaluation on a large number of tasks,  which can be computationally infeasible. CARP-S provides a subset of representative  benchmarking tasks for each scenario to allow for effective evaluation.</p> </li> <li> <p> Minimal Dependencies</p> <p>CARP-S is modular and allows you to only install what you need to run and evaluate what you want. For details, please have a look at the installation instructions.</p> </li> </ul>"},{"location":"changelog/","title":"Version 0.1.0","text":"<ul> <li>Initial version of CARP-S.</li> </ul>"},{"location":"commands/","title":"Commands","text":"<p>You can run a certain problem and optimizer combination directly with Hydra via: <pre><code>python -m carps.run +problem=... +optimizer=... seed=... -m\n</code></pre></p> <p>Another option is to fill the database with all possible combinations of problems and optimizers you would like to run: <pre><code>python -m carps.container.create_cluster_configs +problem=... +optimizer=... -m\n</code></pre></p> <p>Then, run them from the database with: <pre><code>python -m carps.run_from_db \n</code></pre></p> <p>To check whether any runs are missing, you can use the following command. It will create a file <code>runcommands_missing.sh</code> containing the missing runs: <pre><code>python -m carps.utils.check_missing &lt;rundir&gt;\n</code></pre></p> <p>To collect all run data generated by the file logger into csv files, use the following command: <pre><code>python -m carps.analysis.gather_data &lt;rundir&gt;\n</code></pre> The csv files are then located in <code>&lt;rundir&gt;</code>. The <code>logs.csv</code> contain the trial info and values and  the <code>logs_cfg.csv</code> contain the experiment  configuration. The experiments can be matched via the column <code>experiment_id</code>.</p> <p>Experiments with error status (or any other status) can be reset via: <pre><code>python -m carps.utils.database.reset_experiments\n</code></pre></p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation-from-pypi","title":"Installation from PyPI","text":"<p>To install CARP-S, you can simply use <code>pip</code>:</p> <pre><code>conda create -n carps python=3.11\nconda activate carps\npip install carps\n</code></pre> <p>Additionally, you need to install the requirements for the benchmark and optimizer that you want to use. For example, if you want to use the <code>SMAC2.0</code> optimizer and the <code>BBOB</code> benchmark, you need to install the requirements for both of them via:</p> <pre><code>pip install carps[smac,bbob]\n</code></pre> <p>All possible install options for benchmarks are: <pre><code>dummy,bhob,hpob,mfpbench,pymoo,yahpo\n</code></pre></p> <p>All possible install options for optimizers are: <pre><code>dummy,dehb,hebo,nevergrad,optuna,skopt,smac,smac14,synetune\n</code></pre></p> <p>Please note that installing all requirements for all benchmarks and optimizers in a single  environment will not be possible due to conflicting dependencies.</p>"},{"location":"installation/#installation-from-source","title":"Installation from Source","text":"<p>If you want to install from source, you can clone the repository and install CARP-S via:</p> <pre><code>git clone https://github.com/AutoML/CARP-S.git\ncd CARP-S\nconda create -n carps python=3.11\nconda activate carps\n\n# Install for usage\npip install .\n</code></pre> <p>For installing the requirements for the optimizer and benchmark, you can then use the following command: <pre><code>pip install \".[smac,bbob]\"\n</code></pre></p> <p>If you want to install CARP-S for development, you can use the following command: <pre><code>make install-dev\n</code></pre></p>"},{"location":"installation/#additional-steps-for-benchmarks","title":"Additional Steps for Benchmarks","text":"<p>For HPOBench, it is necessary to install the requirements via: <pre><code>bash container_recipes/benchmarks/HPOBench/install_HPOBench.sh\n</code></pre></p> <p>For some benchmarks, it is necessary to download data,  such as surrogate models, in order to run the benchmark: </p> <ul> <li> <p>For HPOB, you can download the surrogate benchmarks with     <pre><code>bash container_recipes/benchmarks/HPOB/download_data.sh\n</code></pre></p> </li> <li> <p>For MFPBench, you can download the surrogate benchmarks with     <pre><code>bash container_recipes/benchmarks/MFPBench/download_data.sh\n</code></pre></p> </li> <li> <p>For YAHPO, you can download the required surrogate benchmarks and meta-data with     <pre><code>bash container_recipes/benchmarks/YAHPO/prepare_yahpo.sh\n</code></pre></p> </li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/automl/CARP-S/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>CARP-S could always use more documentation, whether as part of the official CARP-S docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at github.com/automl/CARP-S/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up <code>CARP-S</code> for local development.</p> <p>Fork the <code>CARP-S</code> repo on GitHub and then clone your fork locally: <pre><code>git clone git@github.com:YOUR_NAME_HERE/CARP-S.git\ncd CARP-S\n</code></pre></p> <p>Install your local copy into a virtualenv. We'll also install <code>pre-commit</code> which runs some code quality checks. <pre><code>python -m venv .venv\npip install -e \".[dev]\"\npre-commit install\n</code></pre></p> <p>Create a branch for local development: <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre></p> <p>Now you can make your changes locally!</p> <p>When you're done making changes, check that your changes pass ruff, including testing other Python versions: <pre><code>python setup.py test or pytest\n</code></pre> Commit your changes and push your branch to GitHub:</p> <p><pre><code>git add .\ngit commit -m \"Your detailed description of your changes.\"\ngit push origin name-of-your-bugfix-or-feature\n</code></pre> Submit a pull request through the GitHub website!</p>"},{"location":"contributing/#local-development","title":"Local Development","text":""},{"location":"contributing/#virtual-environments","title":"Virtual Environments","text":"<p>You can try to install all dependencies into one big environment, but probably there are package clashes. Therefore, you can build one virtual environment for each optimizer-benchmark combination. Either run <code>scripts/build_envs.sh</code> to build all existing combinations or copy the combination and run as needed. It will create an environment with name <code>automlsuite_${OPTIMIZER_CONTAINER_ID}_${BENCHMARK_ID}</code>.</p>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in <code>README.md</code>.</li> <li>The pull request should work for <code>Python 3.9</code> and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"contributing/#testing","title":"Testing","text":"<p>To run a subset of tests: <pre><code>pytest tests/some_file.py  # Run tests only in a certain file\npytest -k \"test_mytest\"  # Find tests with a name matching \"test_mytest\"\n</code></pre></p>"},{"location":"contributing/#deploying","title":"Deploying","text":"<p>A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in <code>CHANGELOG.md</code>).</p> <p>Update the version in <code>pyproject.toml</code>, then run:</p> <pre><code>git tag \"x.y.z\"  # Replace with your version\ngit push\ngit push --tags\n</code></pre>"},{"location":"contributing/contributing-a-benchmark/","title":"Contributing a Benchmark","text":"<p>To add a new benchmark problem to CARP-S, you need to create a Python file that defines a  new problem class. This class should inherit from the <code>Problem</code> class defined in  <code>carps/benchmarks/problem.py</code>. </p> <p>Here's a step-by-step guide for how to add a new benchmark:</p> <ol> <li> <p>Benchmark Python file: Create a new Python file in the <code>carps/benchmarks/</code> directory.  For example, you might name it <code>my_benchmark.py</code>.</p> </li> <li> <p>Define your problem class:  Define a new class that inherits from <code>Problem</code>. This class should implement the <code>configspace</code>  property and the <code>_evaluate</code> method, as these are abstract in the base <code>Problem</code> class.  The <code>configspace</code> property should return a <code>ConfigurationSpace</code> object that defines the  configuration space for your problem. The <code>_evaluate</code> method should take a <code>TrialInfo</code> object  and return a <code>TrialValue</code> object. If your problem requires additional methods, you can implement  them in your class. For example, you might need a method to load data for your problem. </p> </li> <li> <p>Requirements file: Create a requirements file and add the requirements for your benchmark.     The file structure must be     <code>container_recipes/benchmarks/&lt;benchmark_id&gt;/&lt;benchmark_id&gt;_requirements.txt</code>, so for example,    <code>container_recipes/benchmarks/my_benchmark/my_benchmark_requirements.txt</code>. Please specify exact     versions of all requirements! This is very important for reproducibility.</p> </li> <li> <p>Config files: Add config files for the different benchmarking tasks under     <code>carps/configs/problem/my_benchmark/my_benchmark_config_{task}.yaml</code>.     You can use the existing config files as a template.</p> </li> </ol> <p>Here's a basic example of what your <code>my_benchmark.py</code> file might look like:</p> <pre><code>from ConfigSpace import ConfigurationSpace\nfrom carps.benchmarks.problem import Problem\nfrom carps.utils.trials import TrialInfo, TrialValue\n\nclass MyBenchmarkProblem(Problem):\n    def __init__(self, loggers=None):\n        super().__init__(loggers)\n        # Initialize any additional attributes your problem needs here\n\n    @property\n    def configspace(self) -&gt; ConfigurationSpace:\n        # Return a ConfigurationSpace object that defines the configuration space for your problem\n        pass\n\n    def _evaluate(self, trial_info: TrialInfo) -&gt; TrialValue:\n        # Evaluate a trial and return a TrialValue object\n        pass\n\n    # Implement any additional methods your problem needs here\n</code></pre>"},{"location":"contributing/contributing-an-optimizer/","title":"Contributing an Optimizer","text":"<p>To add a new optimizer to CARPS, you need to create a new Python file that defines a new optimizer  class. This class should inherit from the <code>Optimizer</code> class defined in  <code>carps/optimizers/optimizer.py</code>. You can have a look at the  optimizer template  for inspiration.</p> <p>Here's a step-by-step guide for how to add a new optimizer:</p> <ol> <li> <p>Create a new Python file:  Create a new Python file in the <code>carps/optimizers/</code> directory. For example, you might name it <code>my_optimizer.py</code>.</p> </li> <li> <p>Define your optimizer class: Define a new class that inherits from <code>Optimizer</code>. This class should implement the  <code>convert_configspace</code> method, that takes a ConfigSpace configuration space and converts it to  a search space from the optimizer, and a <code>convert_to_trial</code> method, that converts a proposal by  the optimizer to a TrialInfo object. Furthermore, a <code>_setup_optimizer</code> method should be implemented, setting up and returning the optimizer to be used, and a <code>get_current_incumbent</code> method, extracting  the incumbent config and cost. Finally, an <code>ask</code> method is required, that  queries a new trial to evaluate from the optimizer and returns it as a TrialInfo object, and a  <code>tell</code> method, that takes a TrialInfo and TrialValue and updates the optimizer with the results of  the trial. If your optimizer requires additional methods, you can implement them in your class. </p> </li> <li> <p>Requirements file: Create a requirements file and add the requirements for your optimizer.    The file structure must be     <code>container_recipes/optimizers/&lt;optimizer_container_id&gt;/&lt;optimizer_container_id&gt;_requirements.txt</code>,    so for example, <code>container_recipes/optimizers/my_optimizer/my_optimizer_requirements.txt</code>.    Please specify exact versions of all requirements! This is very important for reproducibility.</p> </li> <li> <p>Config files: Add config files for the different optimizers under     <code>carps/configs/optimizer/my_optimizer/my_optimizer_config_{variant}.yaml</code>.     You can use the existing config files as a template.</p> </li> </ol> <p>Here's a basic example of what your <code>my_optimizer.py</code> file might look like:</p> <pre><code>from ConfigSpace import Configuration, ConfigurationSpace\nfrom carps.utils.trials import TrialInfo, TrialValue\nfrom carps.optimizers.optimizer import Optimizer\nfrom carps.utils.types import Incumbent\n\nclass MyOptimizer(Optimizer):\n    def __init__(self, problem, task, loggers=None):\n        super().__init__(problem, task, loggers)\n        # Initialize any additional attributes your optimizer needs here\n\n    def convert_configspace(self, configspace: ConfigurationSpace) -&gt; OptimizerSearchSpace:\n        # Convert ConfigSpace configuration space to search space from optimizer.\n        pass\n\n    def convert_to_trial(self, optimizer_trial: OptimizerTrial) -&gt; TrialInfo:\n        # Convert proposal by optimizer to TrialInfo.\n        pass\n\n    def _setup_optimizer(self) -&gt; Any:\n        # Setup the optimizer.\n        pass\n\n    def get_current_incumbent(self) -&gt; Incumbent:\n        # Extract the incumbent config and cost.\n        pass\n\n    def ask(self) -&gt; TrialInfo:\n        # Ask the optimizer for a new trial to evaluate.\n        pass\n\n    def tell(self, trial_info: TrialInfo, trial_value: TrialValue) -&gt; None:\n        # Tell the optimizer a new trial.\n        pass\n\n    # Implement any additional methods your optimizer needs here\n</code></pre>"},{"location":"guides/containers/","title":"Containers","text":"<p>Warning</p> <p>The usage of containers for benchmarking is still under development.</p> <p>There are two main ways to use this framework: 1. Run everything in the same environment (e.g. to test locally) 2. Build separate Singularity/ Apptainer containers for the optimizer and the benchmark (e.g. to run on a cluster)</p> <p>The first options can allow for faster development, but the second option is more robust and flexible since python or  other package versions do not clash, and eases execution on e.g. a SLURM cluster.</p> <p>The overall benchmarking system works as follows: </p> <p>There are three different containers wrapping different functionality and a shell script controlling these containers.  The <code>HydraInitializer</code> container is responsible for constructing the Hydra configuration,  which is required to initialize the <code>Optimizer</code> and the <code>Benchmark</code> container.  The <code>Benchmark</code> container wraps the actual benchmark to be run and provides two main functionalities via a web service.  First, it allows to get the search space associated with the benchmark.  Second, it answers requests providing a configuration to be evaluated with the corresponding evaluation result. The <code>Optimizer</code> container wraps the optimizer to be benchmarked and interacts with the <code>Benchmark</code> container. Any information required to boot the containers is written to the hard drive by the <code>HydraInitializer</code> container. </p> <p>Note that we provide wrappers for the optimizer and the benchmark interfaces such that when you implement an  optimizer or a benchmark within our benchmarking framework,  you can ignore all aspects of the system just described and simply follow the simple API. </p> <p></p>"},{"location":"guides/containers/#installation","title":"Installation","text":"<ul> <li>Local: Install Apptainer</li> <li>Cluster: Configure Singularity/ Apptainer</li> <li>Setup Database if logging to to the database is required (mysql)</li> </ul>"},{"location":"guides/containers/#containerization","title":"Containerization","text":"<p>To run benchmarking with containers, both the optimizer and benchmark have to be wrapped separately.  We use Singularity/ Apptainer for this purpose. The following example illustrates the principle based on a <code>DummyOptimizer</code> and <code>DummyBenchmark</code>.</p> <p>\ud83d\udca1 You can check the location of the log files of your singularity instances with <code>singularity instance list -l</code>.</p> <p>\u26a0 When creating recipes, take care that the paths are correct. In particular, check relative vs. absolute paths (e.g. benchmarking/... \u274c vs /benchmarking/... \u2714).</p> <p>\ud83d\udca1 You can check the creation time and benchmarking version of a container with <code>singularity inspect &lt;container&gt;</code>.     To check the versions of the installed packages, you can use <code>singularity exec &lt;container&gt; pip freeze</code>.</p>"},{"location":"guides/containers/#noctua2-setup-before-compilation","title":"Noctua2 Setup Before Compilation","text":"<p>Include the following lines in your <code>~/.bashrc</code>:</p> <pre><code>export SINGULARITY_CACHEDIR=$PC2PFS/hpc-prf-intexml/&lt;USER&gt;/.singularity_cache\nexport SINGULARITY_TMPDIR=/dev/shm/intexml&lt;X&gt;\nmkdir /dev/shm/intexml&lt;X&gt; -p\n</code></pre>"},{"location":"guides/containers/#optimizer","title":"Optimizer","text":"<p>A Singularity recipe has to be created for the optimizer, which should be saved in the folder <code>container_recipes</code>. This recipe has the purpose of setting up a container in which the optimizer can be run, e.g., installing the  required packages, setting environment variables, copying files and so on. For the <code>Dummy_Optimizer</code> this is <code>container_recipes/dummy_optimizer/dummy_optimizer.recipe</code>, which you can consult  as a basis for other optimizers.</p> <p>The optimizer then has to be built to an image named after the optimizer id, e.g., <code>DUMMY_Optimizer.sif</code> for the <code>DummyOptimizer</code> using the following command:</p> <pre><code>singularity build containers/optimizers/DUMMY_Optimizer.sif container_recipes/optimizers/DUMMY_Optimizer/DUMMY_Optimizer.recipe\n</code></pre> <p>To facilitate this process, a short script is provided for this purpose, which is however system-specific to Noctua2. It can be run as follows:</p> <pre><code>./compile_noctua2.sh containers/optimizers/DUMMY_Optimizer.sif container_recipes/optimizers/DUMMY_Optimizer/DUMMY_Optimizer.recipe\n</code></pre>"},{"location":"guides/containers/#benchmark","title":"Benchmark","text":"<p>Like for the optimizer, a Singularity recipe has to be created for the benchmark, which should be saved in the folder <code>container_recipes</code> as well.</p> <p>The benchmark image also has to be according to the benchmark id, e.g., <code>DUMMY_Problem.sif</code> for the  <code>DummyBenchmark</code>  using the following command:</p> <pre><code>singularity build containers/benchmarks/DUMMY_Problem.sif container_recipes/benchmarks/DUMMY_Problem/DUMMY_Problem.recipe\n</code></pre> <p>Command for Noctua2:</p> <pre><code>./compile_noctua2.sh containers/benchmarks/DUMMY_Problem.sif container_recipes/benchmarks/DUMMY_Problem/DUMMY_Problem.recipe\n</code></pre>"},{"location":"guides/containers/#running","title":"Running","text":"<p>A third container is needed that handles the hydra config. It does not need to be adjusted for each optimizer or benchmark, but can be used as is. It can be built as follows:</p> <pre><code>singularity build containers/general/runner.sif container_recipes/general/runner.recipe\n</code></pre> <p>Command for Noctua2:</p> <pre><code>./compile_noctua2.sh containers/general/runner.sif container_recipes/general/runner.recipe\n</code></pre> <p>Running the containerized benchmarking system is also system-dependent. An example for Noctua2 is provided in the script <code>start_container_noctua2.sh</code>. It can be run as follows:</p> <pre><code>./start_container_noctua2.sh\n</code></pre> <p>NOTE: This needs to be run in a SLURM-job, so either an interactive job</p> <pre><code>srun --cpus-per-task=2 -p normal --mem=2gb -n 1 --time=00:30:00 --pty bash\n</code></pre> <p>or a job allocated via script.</p> <p>This will pull a job from the database and run it (database needs to be initialized beforehand). To be efficient, this command should eventually be integrated into a SLURM script, which can be submitted to the cluster (e.g. with job arrays).</p>"},{"location":"guides/database/","title":"Database","text":"<p>This document describes how to set up the database for the CARP-S framework and use it for logging experiment results and trajectories.</p> <p>Either SQLite or MySQL can be used as database, which has some slight differences.  Using SQLite is straightforward; you get a local database file but parallel execution is not efficient at all. You configure the used database in the  <code>carps/container/py_experimenter.yaml</code> file by changing the <code>provider</code> to <code>mysql</code> or  <code>sqlite</code>. </p> <p>In any case, before you can start any jobs, the jobs need to be dispatched to the database. To this end, call the file <code>create_cluster_configs.py</code> with the desired hydra arguments. This can be done locally or on the server if you can execute python there directly. If you execute it locally, the database file <code>carps.db</code> will be created in the current directory and  needs to be transferred to the cluster.</p> <pre><code>python carps/container/create_cluster_configs.py +optimizer/DUMMY=config +problem/DUMMY=config 'seed=range(1,21)' --multirun\n</code></pre> <p>If you want to use a personal/local MySQL database, follow these steps:</p> <ol> <li> <p>Setup MySQL (tutorial)</p> </li> <li> <p>Create database via <code>mysql&gt; CREATE DATABASE carps;</code>     Select password as authentification.     Per default, the database name is <code>carps</code>.     It is set in <code>carps/container/py_experimenter.yaml</code>.</p> </li> <li> <p>Add credential file at <code>carps/container/credentials.yaml</code>, e.g. <pre><code>CREDENTIALS:\n  Database:\n    user: root\n    password: &lt;password&gt;\n  Connection:\n    Standard:\n      server: localhost\n</code></pre></p> </li> <li> <p>Set flag not to use ssh server in <code>carps/container/py_experimenter.yaml</code> if you are on your local machine.</p> </li> </ol>"},{"location":"guides/first-steps/","title":"First Steps","text":"<p>First, follow the installation instructions to install CARP-S. As described in the installation guide, make sure to install the requirements for the benchmark and optimizer you would like to run, e.g. <code>SMAC2.0</code> and <code>BBOB</code>.</p> <p>Once the requirements for both an optimizer and a benchmark are installed, you can run one of  the following minimal examples to benchmark <code>SMAC2.0</code> on <code>BBOB</code> directly with Hydra:</p> <pre><code># Run SMAC BlackBoxFacade on certain BBOB problem\npython -m carps.run +optimizer/smac20=blackbox +problem/BBOB=cfg_4_1_4_0 seed=1 task.n_trials=25\n\n# Run SMAC BlackBoxFacade on all available BBOB problems for 10 seeds\npython -m carps.run +optimizer/smac20=blackbox '+problem/BBOB=glob(*)' 'seed=range(1,11)' -m\n</code></pre> <p>For the second command, the Hydra -m (or --multirun) option indicates that multiple runs will be  performed over a range of parameter values. In this case, it's indicating that the benchmarking should be run for all available BBOB problems (+problem/BBOB=glob(*)) and for 10 different  seed values (seed=range(1,11)).</p>"},{"location":"guides/hydra/","title":"Hydra","text":"<p>In CARP-S, Hydra is used for handling the configuration of the benchmarking tasks to be executed. </p> <p>Hydra is a Python framework that simplifies the development of complex applications by enabling  their configuration to be dynamically composed and overridden, which is particularly useful  for applications that require a high degree of customization and flexibility. </p> <p>For more information on Hydra, please refer to the  Hydra documentation.</p>"},{"location":"guides/large-scale-benchmarking/","title":"Large Scale Benchmarking","text":""},{"location":"guides/large-scale-benchmarking/#parallel","title":"Parallel","text":"<p>You can run your optimization via <pre><code>conda run -n automlsuite_DUMMY_Optimizer_DUMMY_Problem python carps/run.py \\\n    +optimizer/DUMMY=config +problem/DUMMY=config \\\n    'seed=range(1,11)' \\\n    +cluster=local -m\n</code></pre> This uses joblib parallelization on your local machine. If you are on a slurm cluster, you can specify <code>+cluster=slurm</code> and adapt this to your needs. Check this page for more launchers, e.g. Ray or RQ besides Joblib and Submitit.</p>"}]}