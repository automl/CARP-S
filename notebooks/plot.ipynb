{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# from carps.analysis.process_data import get_interpolated_performance_df, load_logs, process_logs\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "import carps\n",
    "import carps.analysis\n",
    "import carps.analysis.gather_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "importlib.reload(carps)\n",
    "importlib.reload(carps.analysis.gather_data)\n",
    "from carps.analysis.gather_data import get_interpolated_performance_df, load_set, normalize_logs\n",
    "\n",
    "\n",
    "def print_overview(df_trials: pd.DataFrame) -> None:\n",
    "    print(df_trials.columns)\n",
    "    print(df_trials[\"optimizer_id\"].unique())\n",
    "    print(df_trials[\"benchmark_id\"].unique())\n",
    "    print(df_trials[\"task_id\"].unique())\n",
    "    print(df_trials[\"scenario\"].unique())\n",
    "    print(\"Number of seeds\", df_trials[\"seed\"].nunique())\n",
    "\n",
    "\n",
    "# rundir = \"../runs\"\n",
    "# df, df_cfg = load_logs(rundir=rundir)\n",
    "# df = pd.read_csv(\"../logs_combined.csv\")\n",
    "# df_cfg = pd.read_csv(\"../logs_combined_cfg.csv\")\n",
    "\n",
    "paths = {\n",
    "    \"BBfull\": {\n",
    "        \"full\": [\n",
    "        \"../runs/SMAC3-BlackBoxFacade\",\n",
    "        \"../runs/RandomSearch\",\n",
    "        \"../runs/Nevergrad-CMA-ES\",\n",
    "    ]},\n",
    "    \"MOfull\": {\n",
    "        \"full\": [\"../runs_MO\"]\n",
    "    },\n",
    "    \"BBsubset\": {\n",
    "        \"dev\": [\"../runs_subset_BB/dev\"],\n",
    "        \"test\": [\"../runs_subset_BB/test\"],\n",
    "    },\n",
    "    \"MFsubset\": {\n",
    "        \"dev\": [\"../runs_subset_MF/dev\"],\n",
    "        \"test\": [\"../runs_subset_MF/test\"],\n",
    "    },\n",
    "    \"MOsubset\": {\n",
    "        \"dev\": [\"../runs_subset_MO/dev\"],\n",
    "        \"test\": [\"../runs_subset_MO/test\"],\n",
    "    },\n",
    "    \"MOMFsubset\": {\n",
    "        \"dev\": [\"../runs_subset_MOMF/dev\"],\n",
    "        \"test\": [\"../runs_subset_MOMF/test\"],\n",
    "    },\n",
    "}\n",
    "subset = \"BBsubset\"\n",
    "task_prefix = \"blackbox/20\"\n",
    "\n",
    "# subset = \"MFsubset\"\n",
    "# task_prefix = \"multifidelity/20\"\n",
    "\n",
    "# subset = \"MOsubset\"\n",
    "# task_prefix = \"multiobjective/10\"\n",
    "\n",
    "# subset = \"MOMFsubset\"\n",
    "# task_prefix = \"momf/9\"\n",
    "\n",
    "\n",
    "\n",
    "loaded = [load_set(paths=ps, set_id=set_id) for set_id, ps in paths[subset].items()]\n",
    "df = pd.concat([d for d, _ in loaded]).reset_index(drop=True)\n",
    "df_cfg = pd.concat([d for _, d in loaded]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(df.columns)\n",
    "df = normalize_logs(df)\n",
    "print_overview(df)\n",
    "perf = get_interpolated_performance_df(df)\n",
    "perf_time = get_interpolated_performance_df(df, x_column=\"time_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per task (raw values)\n",
    "import seaborn as sns\n",
    "from carps.analysis.utils import get_color_palette, savefig, setup_seaborn\n",
    "\n",
    "setup_seaborn(font_scale=1.3)\n",
    "palette = get_color_palette(perf)\n",
    "lineplot_kwargs = {\"linewidth\": 3}\n",
    "\n",
    "key_performance = \"trial_value__cost_inc\"\n",
    "x_column = \"n_trials_norm\"\n",
    "source_df = perf.copy()\n",
    "\n",
    "# Calculate the rank of each optimizer for each task. The estimated performance is\n",
    "# the mean of all seeds. We use the interpolated performance, otherwise it is not\n",
    "# possible to plot well with seaborn (seaborn needs a value for each x value).\n",
    "group_keys_estimate = [\"scenario\", \"set\", \"benchmark_id\", \"task_id\", \"optimizer_id\", x_column]\n",
    "df_estimated = source_df.groupby(group_keys_estimate)[key_performance].mean().reset_index()\n",
    "df_rank = df_estimated.copy()\n",
    "df_rank[\"rank\"] = df_estimated.groupby([\"scenario\", \"set\", \"benchmark_id\", \"task_id\", x_column])[key_performance].rank(ascending=True, method=\"min\")\n",
    "csv_filename = Path(f\"data/rank_{subset}.csv\")\n",
    "csv_filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_rank.to_csv(csv_filename, index=False)\n",
    "\n",
    "for gid, gdf in df_rank.groupby([\"scenario\", \"set\"]):\n",
    "    final_rank = gdf[gdf[x_column] == df_rank[x_column].max()].groupby(\"optimizer_id\")[\"rank\"].mean().sort_values()\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax = sns.lineplot(data=gdf, x=x_column, y=\"rank\", hue=\"optimizer_id\", ax=ax, palette=palette, **lineplot_kwargs)\n",
    "    ax.set_xlabel(\"Number of Trials (normalized)\")\n",
    "    ax.set_ylabel(\"Rank (lower is better)\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    sorted_handles_labels = sorted(zip(handles, labels, strict=False), key=lambda x: final_rank[x[1]])\n",
    "    sorted_handles, sorted_labels = zip(*sorted_handles_labels, strict=False)\n",
    "    sorted_labels = [f\"{l} ({final_rank[l]:.1f})\" for l in sorted_labels]\n",
    "    ax.legend(sorted_handles, sorted_labels, loc=\"center left\", bbox_to_anchor=(1.05, 0.5))\n",
    "    # ax.legend(loc=\"center left\", bbox_to_anchor=(1.05, 0.5))\n",
    "    ax.set_title(f\"{gid[0]}: {gid[1]}\")\n",
    "    figure_filename = f\"figures/rank_{subset}_{gid[0]}_{gid[1]}\"\n",
    "    savefig(fig, figure_filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC for each optimizer per task\n",
    "import seaborn as sns\n",
    "from carps.analysis.utils import get_color_palette, savefig, setup_seaborn\n",
    "\n",
    "setup_seaborn(font_scale=1.3)\n",
    "palette = get_color_palette(perf)\n",
    "lineplot_kwargs = {\"linewidth\": 3}\n",
    "\n",
    "key_performance = \"trial_value__cost_inc_log_norm\"\n",
    "x_column = \"n_trials_norm\"\n",
    "source_df = df.copy()\n",
    "\n",
    "def calculate_auc(x: pd.DataFrame) -> float:\n",
    "    return np.trapezoid(x[key_performance], x[x_column])\n",
    "\n",
    "df_auc = source_df.groupby([\"scenario\", \"set\", \"benchmark_id\", \"task_id\", \"optimizer_id\", \"seed\"]).apply(calculate_auc)\n",
    "df_auc.name = \"auc\"\n",
    "df_auc = df_auc.reset_index()\n",
    "\n",
    "for gid, gdf in source_df.groupby([\"scenario\", \"set\"]):\n",
    "    print(gid)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    for optimizer_id, odf in gdf.groupby(\"optimizer_id\"):\n",
    "        ax = sns.ecdfplot(data=odf, x=\"trial_value__cost_inc_log_norm\", ax=ax, label=optimizer_id, color=palette[optimizer_id], **lineplot_kwargs)\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1.05, 0.5))\n",
    "    # ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Logarithm of Incumbent Cost (normalized)\")\n",
    "    ax.set_ylabel(\"CDF\")\n",
    "    ax.set_title(f\"{gid[0]}: {gid[1]}\")\n",
    "    figure_filename = f\"figures/auc_{subset}_{gid[0]}_{gid[1]}\"\n",
    "    savefig(fig, figure_filename)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from carps.analysis.utils import filter_only_final_performance\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "df_final = filter_only_final_performance(df=df)\n",
    "\n",
    "# Normalize by random search performance\n",
    "baseline = \"RandomSearch\"\n",
    "key_performance = \"trial_value__cost_inc\"\n",
    "group_keys = [\"task_id\", \"seed\"]\n",
    "key_performance_new = \"normalized_cost_inc\"\n",
    "def normalize_by_baseline(x: pd.DataFrame, mean_value: float | None = None) -> pd.Series:\n",
    "    if mean_value is None:\n",
    "        baseline_performance = x[x[\"optimizer_id\"] == baseline][key_performance].values[0]\n",
    "    else:\n",
    "        baseline_performance = mean_value\n",
    "    return (x[key_performance] - baseline_performance) / baseline_performance\n",
    "\n",
    "new_df = []\n",
    "mean_values = df_final[df_final[\"optimizer_id\"] == baseline].groupby(\"task_id\")[key_performance].mean().to_dict()\n",
    "for gid, gdf in df_final.groupby(group_keys):\n",
    "    mean_value = None\n",
    "    # mean_value = mean_values.get(gid[0], None)\n",
    "    gdf[key_performance_new] = normalize_by_baseline(gdf, mean_value)\n",
    "    new_df.append(gdf)\n",
    "df_final = pd.concat(new_df).reset_index(drop=True)\n",
    "\n",
    "n_optimizers = df_final[\"optimizer_id\"].nunique()\n",
    "\n",
    "for gid, gdf in df_final.groupby([\"task_id\"]):\n",
    "    try:\n",
    "        task_id = gid[0]\n",
    "        ax = sns.boxplot(data=gdf, y=\"optimizer_id\", x=key_performance_new)\n",
    "        ax.vlines(0, 0 - 0.5, n_optimizers - 0.5, colors=\"black\")\n",
    "        plt.title(task_id)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(gid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def calculate_auc(df: pd.DataFrame, x_column: str = \"n_trials_norm\", perf_column: str = \"trial_value__cost_inc_norm\") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"delta_x\"] = df[x_column].diff()\n",
    "    df[\"delta_y\"] = df[perf_column].diff()\n",
    "    auc = df[\"delta_x\"] * df[perf_column]\n",
    "    return auc.cumsum()\n",
    "\n",
    "for gid, gdf in perf.groupby(by=[\"task_id\", \"optimizer_id\", \"seed\"]):\n",
    "    # print(gid)\n",
    "    # gdf[\"auc\"]\n",
    "    # print(gdf)\n",
    "    # print(calculate_auc(gdf))\n",
    "    # print(np.log(gdf[\"trial_value__cost_inc_norm\"]))\n",
    "    break\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "result_variants = [\n",
    "    \"trial_value__cost_inc\",\n",
    "    \"trial_value__cost_inc_norm\",\n",
    "    \"trial_value__cost_inc_norm_log\",\n",
    "    \"trial_value__cost_inc_log\",\n",
    "    \"trial_value__cost_inc_log_norm\",\n",
    "]\n",
    "\n",
    "\n",
    "for gid, gdf in df.groupby(\"task_id\"):\n",
    "    print(gid, gdf[\"trial_value__cost_inc\"].min(), gdf[\"trial_value__cost_inc\"].max())\n",
    "    if \"yahpo\" in gid:\n",
    "        for result_variant in result_variants:\n",
    "            ax = sns.histplot(gdf[result_variant], bins=100)\n",
    "            ax.set_title(f\"{gid}: {result_variant}\")\n",
    "            plt.show()\n",
    "        break\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time\n",
    "from carps.analysis.utils import filter_only_final_performance\n",
    "\n",
    "df_final = filter_only_final_performance(df=df)\n",
    "print(\"Runtime\", subset, df_final[\"time\"].sum() / 60 / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import carps\n",
    "import carps.analysis\n",
    "import carps.analysis.plot_ranking\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from carps.analysis.utils import setup_seaborn\n",
    "\n",
    "importlib.reload(carps)\n",
    "importlib.reload(carps.analysis)\n",
    "importlib.reload(carps.analysis.plot_ranking)\n",
    "from carps.analysis.plot_ranking import plot_ranking\n",
    "\n",
    "setup_seaborn(font_scale=1)\n",
    "\n",
    "rank_results = {}\n",
    "for gid, gdf in df.groupby(by=[\"scenario\", \"set\"]):\n",
    "    print(\"-\"*100)\n",
    "    print(gid)\n",
    "    scenario, set_id = gid\n",
    "    perf_col = \"trial_value__cost_inc_log_norm\"\n",
    "    if len(gdf) > 0:\n",
    "        rank_result = plot_ranking(gdf=gdf, scenario=scenario, set_id=set_id, perf_col=perf_col)\n",
    "        rank_results[gid] = rank_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fig, axes = plot_interval_estimates(performance_data=perf, load_from_pickle=False, figure_filename=\"figures/plot_interval_estimates.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from carps.analysis.performance_over_time import get_color_palette, plot_performance_over_time, savefig, setup_seaborn\n",
    "\n",
    "lineplot_kwargs = {\"linewidth\": 3}\n",
    "for gid, gdf in perf.groupby(by=[\"scenario\", \"set\"]):\n",
    "    gid = \"_\".join(gid)\n",
    "    print(gid)\n",
    "    fig, ax = plot_performance_over_time(\n",
    "        df=gdf,\n",
    "        x=\"n_trials_norm\",\n",
    "        y=\"trial_value__cost_inc_norm\",\n",
    "        hue=\"optimizer_id\",\n",
    "        figure_filename=f\"figures/perf_over_time/performance_over_time_{gid}_trials\",\n",
    "        figsize=(6,4),\n",
    "        **lineplot_kwargs\n",
    "    )\n",
    "print(\"plot over time\")\n",
    "for gid, gdf in perf_time.groupby(by=[\"scenario\", \"set\"]):\n",
    "    gid = \"_\".join(gid)\n",
    "    print(gid)\n",
    "    fig, ax = plot_performance_over_time(\n",
    "        df=gdf,\n",
    "        x=\"time_norm\",\n",
    "        y=\"trial_value__cost_inc_norm\",\n",
    "        hue=\"optimizer_id\",\n",
    "        figure_filename=f\"figures/perf_over_time/performance_over_time_{gid}_elapsed\",\n",
    "        figsize=(6,4),\n",
    "        **lineplot_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per benchmark\n",
    "from carps.analysis.performance_over_time import plot_performance_over_time\n",
    "from carps.analysis.utils import get_color_palette, savefig\n",
    "\n",
    "palette = get_color_palette(perf)\n",
    "lineplot_kwargs = {\"linewidth\": 3}\n",
    "\n",
    "for gid, gdf in perf.groupby(by=[\"scenario\", \"set\", \"benchmark_id\"]):\n",
    "    gid = \"_\".join(gid)\n",
    "    # Iterations\n",
    "    figure_filename = f\"figures/perf_over_time/performance_over_time_trials_{gid}\"\n",
    "    fig, ax = plot_performance_over_time(df=gdf, x=\"n_trials_norm\", y=\"trial_value__cost_inc_norm\", hue=\"optimizer_id\", figure_filename=figure_filename, figsize=(6,4), **lineplot_kwargs)\n",
    "    ax.set_title(gid)\n",
    "    savefig(fig, figure_filename)\n",
    "    plt.show()\n",
    "\n",
    "# Elapsed time\n",
    "for gid, gdf in perf_time.groupby(by=[\"scenario\", \"set\", \"benchmark_id\"]):\n",
    "    gid = \"_\".join(gid)\n",
    "    figure_filename = f\"figures/perf_over_time/performance_over_time_elapsed_{gid}\"\n",
    "    fig, ax = plot_performance_over_time(df=gdf, x=\"time_norm\", y=\"trial_value__cost_inc_norm\", hue=\"optimizer_id\", figure_filename=figure_filename, figsize=(6,4), **lineplot_kwargs)\n",
    "    ax.set_title(gid)\n",
    "    savefig(fig, figure_filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per task (raw values)\n",
    "from carps.analysis.performance_over_time import plot_performance_over_time\n",
    "from carps.analysis.utils import get_color_palette, savefig\n",
    "\n",
    "palette = get_color_palette(perf)\n",
    "lineplot_kwargs = {\"linewidth\": 3}\n",
    "\n",
    "for gid, gdf in df.groupby(by=[\"scenario\", \"set\", \"benchmark_id\", \"task_id\"]):\n",
    "    gid = \"_\".join(gid)\n",
    "    gdf = gdf[gdf[\"n_trials_norm\"] >= 0.5]\n",
    "    # Iterations\n",
    "    figure_filename = f\"figures/perf_over_time/performance_over_time_trials_{gid}\"\n",
    "    fig, ax = plot_performance_over_time(df=gdf, x=\"n_trials_norm\", y=\"trial_value__cost_inc\", hue=\"optimizer_id\", figure_filename=figure_filename, figsize=(6,4), **lineplot_kwargs)\n",
    "    ax.set_title(gid)\n",
    "    savefig(fig, figure_filename)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from carps.analysis.final_performance import plot_final_performance_boxplot, plot_final_performance_violinplot\n",
    "from carps.analysis.utils import filter_only_final_performance\n",
    "\n",
    "boxplot_kwargs = {}\n",
    "\n",
    "for gid, gdf in perf.groupby(by=[\"scenario\", \"set\"]):\n",
    "    print(gid)\n",
    "    fig, ax = plot_final_performance_boxplot(\n",
    "        df=gdf,\n",
    "        x=\"trial_value__cost_inc_norm\", y=\"optimizer_id\", hue=\"optimizer_id\", figure_filename=f\"figures/final_perf/final_performance_boxplot_{gid}.pdf\", figsize=(6,4), **boxplot_kwargs)\n",
    "    fig, ax = plot_final_performance_violinplot(\n",
    "        df=gdf,\n",
    "        x=\"trial_value__cost_inc_norm\", y=\"optimizer_id\", hue=\"optimizer_id\", figure_filename=f\"figures/final_perf/final_performance_violinplot_{gid}.pdf\", figsize=(6,4), **boxplot_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = df[\"n_trials_norm\"].max()\n",
    "error = df.groupby(by=[\"benchmark_id\", \"task_id\", \"optimizer_id\", \"seed\"])[\"n_trials_norm\"].apply(lambda x: not np.isclose(x.max(), max_val))\n",
    "error = error[error]\n",
    "for i in error.index:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carpsexp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
